# Bias Audit Report: Financial Credit Scoring ğŸ¦âš–ï¸



## Developed by HerTech Hive

This repository contains a comprehensive bias audit and mitigation framework for financial credit scoring models. The project specifically investigates Age-based discrimination, evaluating whether younger applicants are unfairly disadvantaged by automated lending algorithms.

# ğŸ“Œ Project Overview
Automated credit scoring models can inadvertently inherit historical biases, leading to "Disparate Impact" on protected demographic groups. This audit identifies these patterns, validates them statistically, and implements technical interventions to align the model with Responsible AI principles.

# ğŸ¯ Objectives
Identify Bias Patterns: Analyze the correlation between age and credit approval outcomes.

Quantitative Audit: Compute fairness metrics (Selection Rate, False Negative Rate, and Demographic Parity).

Statistical Validation: Use Chi-Squared testing to prove the significance of discovered biases.

Bias Mitigation: Apply Reweighing and Threshold Moving strategies to ensure equitable lending.

Visual Transparency: Provide comparative analytics of the model performance before and after intervention.

# ğŸ“Š Dataset Summary
The audit analyzes a credit scoring dataset with the following key features:

Protected Attribute: Age (Binary: Young < 25 vs. Mature 25+).

Target Variable: Credit Score (Binary: "Good" vs. "Standard/Poor").

Financial Features: Annual Income, Monthly Salary, Number of Bank Accounts/Credit Cards, Interest Rates, Debt, and Utilization Ratios.

# ğŸ› ï¸ Audit Methodology
## Phase 1: Bias Detection
We utilize a multi-pronged approach to find hidden biases:

Proxy Analysis: Checking if features like "Number of Credit Cards" are secretly acting as a proxy for Age.

Disparate Impact Ratio: Calculating the ratio of approval rates between young and mature groups.

Note: A ratio below 0.80 indicates potential legal and ethical bias.

Intersectional Analysis: Examining how Age interacts with Job Type/Occupation to create "double disadvantage."

Temporal & Sampling Bias: Ensuring the data isn't skewed by specific months or collection periods.

## Phase 2: Mitigation Strategies
To correct the identified bias, the following techniques are implemented:

Reweighing (Pre-processing): Assigning higher weights to underrepresented positive outcomes in the "Young" group during model training.

Threshold Moving (Post-processing): Adjusting the classification thresholds for different age groups to equalize opportunity.

# ğŸ“ˆ Key Results
The audit successfully identifies the "Approval Gap" and demonstrates how mitigation techniques bring the model into compliance with the 80% Fairness Rule.

Metric	Baseline Model	Mitigated (Reweighing)	Mitigated (Threshold)
Disparate Impact Ratio	Detected < 0.8	Improved	Optimized
Statistical Significance	p < 0.05	-	-
# ğŸš€ How to Use
Environment Setup: Ensure you have pandas, numpy, matplotlib, seaborn, scipy, and sklearn installed.

Data: Place credit_score_cleaned_train.csv in the root directory.

Run Audit: Execute the notebook sections sequentially to generate the bias report and apply the fixes.

# âš–ï¸ Ethical Recommendations
Continuous Monitoring: Fairness is not a one-time check; models should be audited quarterly.

Human-in-the-Loop: Automated denials for "Young" applicants should be subject to manual review in borderline cases.

Transparency: Stakeholders should be provided with the "Disparate Impact" reports generated by this tool.

